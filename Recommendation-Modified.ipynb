{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:30:54.817594Z",
     "start_time": "2019-11-21T18:30:53.856303Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:30:54.833770Z",
     "start_time": "2019-11-21T18:30:54.819049Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('./Desktop/UMass/Courses/STAT535/Dataset/ratings.csv')\n",
    "users = pd.read_csv('./Desktop/UMass/Courses/STAT535/Dataset/users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:30:54.839825Z",
     "start_time": "2019-11-21T18:30:54.835259Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_datasets(ratings):\n",
    "    unique_users = ratings.userID.unique()\n",
    "    user_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.userID.map(user_index)\n",
    "    \n",
    "    unique_movies = ratings.movieID.unique()\n",
    "    movie_index = {old: new for new, old in enumerate(unique_movies)}\n",
    "    new_movies = ratings.movieID.map(movie_index)\n",
    "    \n",
    "    num_users = unique_users.shape[0]\n",
    "    num_movies = unique_movies.shape[0]\n",
    "    \n",
    "    X = pd.DataFrame({'userID': new_users, 'movieID': new_movies})\n",
    "    y = ratings[['rating']]\n",
    "    \n",
    "    return num_users, num_movies, X, y, (user_index, movie_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:30:54.848746Z",
     "start_time": "2019-11-21T18:30:54.841779Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReviewsIterator:    \n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        \n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "            \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:30:54.853760Z",
     "start_time": "2019-11-21T18:30:54.850435Z"
    }
   },
   "outputs": [],
   "source": [
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in ReviewsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:30:54.868985Z",
     "start_time": "2019-11-21T18:30:54.855030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 2353 users, 1465 movies\n",
      "Dataset shape: (31620, 2)\n",
      "Target shape: (31620, 1)\n"
     ]
    }
   ],
   "source": [
    "n, m, X, y, _ = create_datasets(ratings)\n",
    "minmax = ratings.rating.min(), ratings.rating.max()\n",
    "print(f'Embeddings: {n} users, {m} movies')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:30:54.877727Z",
     "start_time": "2019-11-21T18:30:54.870109Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_valid)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:30:54.890024Z",
     "start_time": "2019-11-21T18:30:54.879388Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors=50, embedding_dropout=0.02, hidden=10, dropouts=0.2):\n",
    "        super().__init__()\n",
    "        n_last = hidden[-1]\n",
    "            \n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        self.m = nn.Embedding(n_movies, n_factors)\n",
    "        self.drop = nn.Dropout(embedding_dropout)\n",
    "        self.hidden = nn.Sequential(*list(self.gen_layers(n_factors * 2, hidden, dropouts)))\n",
    "        self.fc = self.xavier_init(nn.Linear(n_last, 1))\n",
    "        self.xavier_all_hidden()\n",
    "        \n",
    "    def xavier_all_hidden(self):                \n",
    "        self.u.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.m.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.hidden.apply(self.xavier_init)\n",
    "        \n",
    "    def xavier_init(self, layer):\n",
    "        if type(layer) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "            layer.bias.data.fill_(0.01)\n",
    "        return layer\n",
    "    \n",
    "    def gen_layers(self, n_in, hidden, dropouts):\n",
    "        torch.manual_seed(134)\n",
    "        for n_out, rate in zip(hidden, dropouts):\n",
    "            yield nn.Linear(n_in, n_out)\n",
    "            yield nn.ReLU()\n",
    "            if rate is not None and rate > 0.:\n",
    "                yield nn.Dropout(rate)\n",
    "            n_in = n_out\n",
    "        \n",
    "    def forward(self, users, movies, minmax=None):\n",
    "        features = torch.cat([self.u(users), self.m(movies)], dim=1)\n",
    "        x = self.drop(features)\n",
    "        x = self.hidden(x)\n",
    "        out = torch.sigmoid(self.fc(x))\n",
    "        if minmax is not None:\n",
    "            min_rating, max_rating = minmax\n",
    "            out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n",
    "        return out\n",
    "#         return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:30:54.919361Z",
     "start_time": "2019-11-21T18:30:54.891561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingNet(\n",
       "  (u): Embedding(2353, 150)\n",
       "  (m): Embedding(1465, 150)\n",
       "  (drop): Dropout(p=0.05, inplace=False)\n",
       "  (hidden): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=500, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=500, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbeddingNet(n_users=n, n_movies=m, \n",
    "    n_factors=150, hidden=[500, 500, 500], \n",
    "    embedding_dropout=0.05, dropouts=[0.5, 0.5, 0.25])\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:20:20.707243Z",
     "start_time": "2019-11-21T19:20:04.657497Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/100] train: 1.2046 - val: 1.1696\n",
      "loss improvement on epoch: 2\n",
      "[002/100] train: 1.0185 - val: 0.9674\n",
      "loss improvement on epoch: 3\n",
      "[003/100] train: 0.8493 - val: 0.9084\n",
      "loss improvement on epoch: 4\n",
      "[004/100] train: 0.7570 - val: 0.8597\n",
      "[005/100] train: 0.7107 - val: 0.8602\n",
      "loss improvement on epoch: 6\n",
      "[006/100] train: 0.6836 - val: 0.8566\n",
      "loss improvement on epoch: 7\n",
      "[007/100] train: 0.6615 - val: 0.8546\n",
      "[008/100] train: 0.6386 - val: 0.8713\n",
      "[009/100] train: 0.6203 - val: 0.8701\n",
      "[010/100] train: 0.6075 - val: 0.8801\n",
      "[011/100] train: 0.5944 - val: 0.8758\n",
      "[012/100] train: 0.5791 - val: 0.8869\n",
      "[013/100] train: 0.5633 - val: 0.8952\n",
      "[014/100] train: 0.5419 - val: 0.8930\n",
      "[015/100] train: 0.5166 - val: 0.9362\n",
      "[016/100] train: 0.4836 - val: 0.9382\n",
      "[017/100] train: 0.4513 - val: 0.9664\n",
      "early stopping after epoch 017\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(134)\n",
    "np.random.seed(0)\n",
    "\n",
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 2000\n",
    "n_epochs = 100\n",
    "patience = 10\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "\n",
    "net = EmbeddingNet(\n",
    "    n_users=n, n_movies=m, \n",
    "    n_factors=150, hidden=[500, 500, 500], \n",
    "    embedding_dropout=0.05, dropouts=[0.5, 0.5, 0.25])\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "# iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "    \n",
    "    for phase in ('train', 'val'):\n",
    "        training = phase == 'train'\n",
    "        running_loss = 0.0\n",
    "#         n_batches = 0\n",
    "        \n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = batch\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        \n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "                \n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Mynet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T18:52:06.132075Z",
     "start_time": "2019-11-21T18:52:06.120926Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class myNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myNet, self).__init__()\n",
    "        self.EMB_OUTPUT_DIM = 150\n",
    "        self.CATEGORY_DIM = 6\n",
    "        self.L1 = self.EMB_OUTPUT_DIM * 2\n",
    "        self.L2 = 500\n",
    "        self.L3 = 500\n",
    "        self.L4 = 500\n",
    "        self.L5 = 1\n",
    "        self.dropout = nn.Dropout(p=0.02, inplace=False)\n",
    "        \n",
    "        self.emb1 = nn.Embedding(2353, self.EMB_OUTPUT_DIM)\n",
    "        self.emb3 = nn.Embedding(1465, self.EMB_OUTPUT_DIM)\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(self.L1, self.L2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.L2, self.L3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.L3, self.L4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.emb1.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.emb3.weight.data.uniform_(-0.05, 0.05) \n",
    "#         self.fc1 = self.dropout(nn.Linear(self.L1, self.L2))\n",
    "#         self.fc2 = self.dropout(nn.Linear(self.L2, self.L3))\n",
    "#         self.fc3 = nn.Linear(self.L3, self.L4)\n",
    "        self.fc4 = self.init(nn.Linear(self.L4, 1))\n",
    "        self.hidden.apply(self.init)\n",
    "        \n",
    "    def init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        return m\n",
    "\n",
    "    def forward(self, users, movies):\n",
    "#         categorical_x = x[:, :self.CATEGORY_DIM].long()\n",
    "#         numerical_x = x[:, self.CATEGORY_DIM:].float()\n",
    "#         numerical_x.requires_grad = False\n",
    "#         embedding_list = [self.embedding[i](categorical_x[:, i]) for i in range(6)]\n",
    "        embedding_list = [self.emb1(users), self.emb3(movies)]\n",
    "        embedding_x = torch.cat(embedding_list, dim=1)    \n",
    "# #         feature_x = torch.cat((embedding_x, numerical_x), dim=-1)\n",
    "        feature_x = self.dropout(embedding_x)\n",
    "        output_x = self.hidden(feature_x)\n",
    "        output_x = torch.sigmoid(self.fc4(output_x))\n",
    "        output_x = output_x * (5 - 1 + 1) + 1 -0.5\n",
    "        return output_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### mynet + non-dataloader + working training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T21:54:19.276028Z",
     "start_time": "2019-11-21T21:54:04.376181Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/100] train: 1.1934 - val: 1.1564\n",
      "loss improvement on epoch: 2\n",
      "[002/100] train: 0.9952 - val: 0.9370\n",
      "loss improvement on epoch: 3\n",
      "[003/100] train: 0.8261 - val: 0.8868\n",
      "loss improvement on epoch: 4\n",
      "[004/100] train: 0.7423 - val: 0.8570\n",
      "[005/100] train: 0.7045 - val: 0.8611\n",
      "loss improvement on epoch: 6\n",
      "[006/100] train: 0.6769 - val: 0.8534\n",
      "[007/100] train: 0.6523 - val: 0.8670\n",
      "[008/100] train: 0.6335 - val: 0.8816\n",
      "[009/100] train: 0.6162 - val: 0.8804\n",
      "[010/100] train: 0.5978 - val: 0.8819\n",
      "[011/100] train: 0.5845 - val: 0.8831\n",
      "[012/100] train: 0.5643 - val: 0.9005\n",
      "[013/100] train: 0.5444 - val: 0.8968\n",
      "[014/100] train: 0.5210 - val: 0.9064\n",
      "[015/100] train: 0.4843 - val: 0.9325\n",
      "[016/100] train: 0.4500 - val: 0.9615\n",
      "early stopping after epoch 016\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(134)\n",
    "np.random.seed(0)\n",
    "\n",
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 2000\n",
    "n_epochs = 100\n",
    "patience = 10\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "\n",
    "# net = EmbeddingNet(\n",
    "#     n_users=n, n_movies=m, \n",
    "#     n_factors=150, hidden=[500, 500, 500], \n",
    "#     embedding_dropout=0.05, dropouts=[0.5, 0.5, 0.25])\n",
    "\n",
    "net = myNet()\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "# iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "    \n",
    "    for phase in ('train', 'val'):\n",
    "        training = phase == 'train'\n",
    "        running_loss = 0.0\n",
    "#         n_batches = 0\n",
    "        \n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = batch\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:, 0], x_batch[:, 1])\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        \n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "                \n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### mynet + non-dataloader + my training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T22:14:56.259356Z",
     "start_time": "2019-11-21T22:14:31.734969Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: \n",
      "training loss: 1.2541, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 1: \n",
      "training loss: 5.0162, validation loss: 1.2233, time passed: .2fs\n",
      "\n",
      "Epoch 2: \n",
      "training loss: 1.0679, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 2: \n",
      "training loss: 4.2715, validation loss: 1.0115, time passed: .2fs\n",
      "\n",
      "Epoch 3: \n",
      "training loss: 0.8914, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 3: \n",
      "training loss: 3.5655, validation loss: 0.9509, time passed: .2fs\n",
      "\n",
      "Epoch 4: \n",
      "training loss: 0.7982, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 4: \n",
      "training loss: 3.1926, validation loss: 0.9102, time passed: .2fs\n",
      "\n",
      "Epoch 5: \n",
      "training loss: 0.7497, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 5: \n",
      "training loss: 2.9988, validation loss: 0.9111, time passed: .2fs\n",
      "\n",
      "Epoch 6: \n",
      "training loss: 0.7180, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 6: \n",
      "training loss: 2.8721, validation loss: 0.9057, time passed: .2fs\n",
      "\n",
      "Epoch 7: \n",
      "training loss: 0.6951, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 7: \n",
      "training loss: 2.7804, validation loss: 0.8919, time passed: .2fs\n",
      "\n",
      "Epoch 8: \n",
      "training loss: 0.6739, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 8: \n",
      "training loss: 2.6955, validation loss: 0.9140, time passed: .2fs\n",
      "\n",
      "Epoch 9: \n",
      "training loss: 0.6568, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 9: \n",
      "training loss: 2.6270, validation loss: 0.9177, time passed: .2fs\n",
      "\n",
      "Epoch 10: \n",
      "training loss: 0.6358, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 10: \n",
      "training loss: 2.5434, validation loss: 0.9269, time passed: .2fs\n",
      "\n",
      "Epoch 11: \n",
      "training loss: 0.6282, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 11: \n",
      "training loss: 2.5127, validation loss: 0.9279, time passed: .2fs\n",
      "\n",
      "Epoch 12: \n",
      "training loss: 0.6048, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 12: \n",
      "training loss: 2.4193, validation loss: 0.9338, time passed: .2fs\n",
      "\n",
      "Epoch 13: \n",
      "training loss: 0.5963, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 13: \n",
      "training loss: 2.3850, validation loss: 0.9436, time passed: .2fs\n",
      "\n",
      "Epoch 14: \n",
      "training loss: 0.5747, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 14: \n",
      "training loss: 2.2988, validation loss: 0.9500, time passed: .2fs\n",
      "\n",
      "Epoch 15: \n",
      "training loss: 0.5473, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 15: \n",
      "training loss: 2.1890, validation loss: 0.9835, time passed: .2fs\n",
      "\n",
      "Epoch 16: \n",
      "training loss: 0.5117, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 16: \n",
      "training loss: 2.0466, validation loss: 1.0007, time passed: .2fs\n",
      "\n",
      "Epoch 17: \n",
      "training loss: 0.4771, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 17: \n",
      "training loss: 1.9084, validation loss: 1.0139, time passed: .2fs\n",
      "\n",
      "Epoch 18: \n",
      "training loss: 0.4464, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 18: \n",
      "training loss: 1.7857, validation loss: 1.0490, time passed: .2fs\n",
      "\n",
      "Epoch 19: \n",
      "training loss: 0.4164, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 19: \n",
      "training loss: 1.6655, validation loss: 1.0538, time passed: .2fs\n",
      "\n",
      "Epoch 20: \n",
      "training loss: 0.3808, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 20: \n",
      "training loss: 1.5232, validation loss: 1.0978, time passed: .2fs\n",
      "\n",
      "Epoch 21: \n",
      "training loss: 0.3563, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 21: \n",
      "training loss: 1.4254, validation loss: 1.0727, time passed: .2fs\n",
      "\n",
      "Epoch 22: \n",
      "training loss: 0.3342, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 22: \n",
      "training loss: 1.3370, validation loss: 1.0703, time passed: .2fs\n",
      "\n",
      "Epoch 23: \n",
      "training loss: 0.3171, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 23: \n",
      "training loss: 1.2684, validation loss: 1.0822, time passed: .2fs\n",
      "\n",
      "Epoch 24: \n",
      "training loss: 0.2996, validation loss: 0.0000, time passed: .2fs\n",
      "\n",
      "Epoch 24: \n",
      "training loss: 1.1983, validation loss: 1.0766, time passed: .2fs\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b2a129edfbda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = EmbeddingNet(n_users=2353, n_movies=1465, \n",
    "    n_factors=150, hidden=[500, 500, 500], \n",
    "    embedding_dropout=0.05, dropouts=[0.5, 0.5, 0.25])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "EPOCHS = 300\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "# training\n",
    "# start = time.time()\n",
    "\n",
    "for epo in range(EPOCHS):\n",
    "    if (epo + 1) % 10 == 0:\n",
    "        learning_rate /= 2\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_loss_val = 0\n",
    "    \n",
    "    for phase in ('train', 'val'):\n",
    "        training = phase == 'train'\n",
    "        \n",
    "        for i, batch in enumerate(batches(*datasets[phase], shuffle=training, bs=2000)):\n",
    "    #         temp_training, temp_validation = _data\n",
    "            # training inputs\n",
    "    #         inputs, targets = temp_training\n",
    "            x_batch, y_batch = batch\n",
    "\n",
    "            optimizer.zero_grad()        \n",
    "            outputs = net(x_batch[:,0], x_batch[:,1], minmax=[1,5])\n",
    "            batch_loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # validation inputs\n",
    "    #         val_inputs, val_targets = temp_validation\n",
    "\n",
    "    #         val_outputs = net(val_inputs[:,0], val_inputs[:,1], minmax=[1,5])\n",
    "    #         batch_loss_val = criterion(val_outputs, val_targets)\n",
    "\n",
    "            # backpropagation\n",
    "            if phase == 'train':\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += batch_loss.item()\n",
    "            else:\n",
    "                running_loss_val += batch_loss.item()\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / (i+1)\n",
    "        epoch_loss_val = running_loss_val / (i+1)\n",
    "    #     end = time.time()\n",
    "        print('Epoch %d: \\ntraining loss: %.4f, validation loss: %.4f, time passed: .2fs\\n'%\n",
    "              (epo+1, epoch_loss, epoch_loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
